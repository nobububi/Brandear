{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kaggle project.\n",
    "\n",
    "- work in progress.\n",
    "    1. one hot encoded \"session_title\"(for stacking with lasso)(TBD)\n",
    "    2. how many games and activities are played during when children play.\n",
    "        - check if children play from game to game without rest.\n",
    "        - how to set the threshold?\n",
    "        - investigate it interactively.\n",
    "\n",
    "# dsb-2019\n",
    "\n",
    "\n",
    "- 12/05\n",
    "    - shapã§ä½•ãŒã©ã†åŠ¹ã„ã¦ã„ã‚‹ã®ã‹ç¢ºèªã™ã‚‹ã€‚\n",
    "    - é›£ã—ã„ã€‚â†’è‡ªä½œé–¢æ•°ã«å¯¾ã—ã¦å‡¦ç†ã‚’å®Ÿè¡Œã§ããªã„ã€‚\n",
    "    - simpleãªlgbã§å®Ÿè¡Œã—ã¦æ¤œè¨¼ã—ã¦ã¿ã‚‹ã€‚\n",
    "- 12/06\n",
    "    - ãƒ©ãƒ³ãƒ€ãƒ æ€§ãŒseed averagingã—ã¦ã‚‚å­˜åœ¨ã—ã¦ã—ã¾ã†ã€‚\n",
    "    - lightgbmã«ä½¿ã†ã‚«ãƒ©ãƒ ãŒå®Ÿè¡Œæ®µéšã§å¤‰ã‚ã£ã¦ã—ã¾ã†ï¼Ÿä½•ã‚’ã©ã‚“ã ã‘ä½¿ã£ã¦ã„ãƒ«ã‚«ã€‚\n",
    "    ç‰¹å¾´é‡é‡è¦åº¦ã‚’å¿…ãšå–ã£ã¦ãã¦â†’æ¨™æº–åå·®ãƒ»å¹³å‡ã‚’å–ã‚ŠãŸã„ã€‚\n",
    "    - transformeréå­¦ç¿’èµ·ã“ã—ã†ã‚‹ã‹ã‚‰ã‚„ã‚ãŸã„ã€‚importancetopå±¤ã„ãªã„ã€‚ï¼ˆå‡ºç¾å›æ•°å°‘ãªã„ã‹ã‚‰ã‹ã‚‚ã€‚ï¼‰\n",
    "\n",
    "\n",
    "- 1/1\n",
    "    - levelæ¯ã«ãªã‚“ã®ã‚²ãƒ¼ãƒ ã‚’çµŒé¨“ã—ãŸã‹ï¼ˆï¼game_session_countï¼‰ã‚’ç‰¹å¾´é‡ã«åŠ ãˆã‚‹ã€‚\n",
    "    - æœ›ã¾ã—ã„ãƒ«ãƒ¼ãƒˆã‹å¦ã‹ã®æŒ‡æ¨™ã®è¿½åŠ \n",
    "    [TBD]\n",
    "    0 or Otherã®è­˜åˆ¥ã¯ã†ã¾ãè¡Œã£ã¦ã„ã‚‹å°è±¡ãŒã‚ã‚‹ã€‚\n",
    "    å˜ç´”ãªã‚¤ãƒ¼ã‚¸ãƒ¼ãƒŸã‚¹ãŒå¤šã„ã‹å¦ã‹ã®è­˜åˆ¥ã‚’ã©ã†ã™ã‚‹ã¹ãã‹ï¼Ÿ\n",
    "        ç‰¹å¾´é‡ã®é¸å®šã‚’ã™ã¹ã\n",
    "        countã§ã¯ãªãã€å‰²åˆã‚‚ã—ãã¯time\n",
    "    - 4â—ç³»ã®activityã‚’ã¡ã‚ƒã‚“ã¨ã‚„ã£ã¦ã„ã‚‹ã‹ï¼Ÿï¼ˆevent_dataã®è©³ç´°ã‚’è¿½ã†ï¼‰\n",
    "        - 4000ç³»ã§ã‚ã‚Œã°ãªã‚“ã§ã‚‚ã„ã„\n",
    "        - session_countæ•°ã§ã¯ãªãã€å‰²åˆã«å¤‰ãˆã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚\n",
    "- 1/3\n",
    "    - stackingã™ã‚‹ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã‚’cross validationå†…éƒ¨ã«ã‚»ãƒƒãƒˆ\n",
    "    - n_seeds = 10ã§ã‚»ãƒƒãƒˆã—ã¦ã‚„ã£ã¦ã„ã‚‹ãŒã€åŒã˜ã“ã¨ã‚’10å›ãƒ«ãƒ¼ãƒ—ã—ã¦ã„ã‚‹ã ã‘ã«ãªã£ã¦ã„ã‚‹ã€‚\n",
    "    - Lightgbmã®random_seedå¼•æ•°ã«ã¤ã„ã¦èª¿ã¹ã‚‹ã€‚\n",
    "    - <font color=\"red\">confusion_matrixã‚’å‡ºåŠ›ã™ã‚‹</font>\n",
    "        ã§ãã¦ã„ãªã„\n",
    "    - lightgbmã®æŒ™å‹•ãŒkernelç’°å¢ƒã§ç•°ãªã‚‹ã€‚\n",
    "        - versionã®é•ã„ã ã‚ã†ï¼šæ¬ æã—ãŸã‚«ãƒ©ãƒ ãŒã‚ã‚‹orã‚«ãƒ©ãƒ ãƒãƒ¼ãƒ ã«ç©ºç™½æ–‡å­—ãŒã‚ã‚‹ã‚±ãƒ¼ã‚¹ã‚’è¨±å®¹ã—ãªããªã£ãŸã¨æ€ã‚ã‚Œã‚‹\n",
    "    - èª¬æ˜ã‚’ã¡ã‚ƒã‚“ã¨èãå­ã‹å¦ã‹ã¯å–ã‚Œã‚‹ã‹ï¼Ÿ\n",
    "\n",
    "- 1/5 division by total_countã‚’ã‚„ã‚‹ã¹ãã§ã¯ï¼Ÿ (discussionã‹ã‚‰)\n",
    "\n",
    "\n",
    "\n",
    "- 1/22\n",
    "    - test dataãŒãƒ©ãƒ³ãƒ€ãƒ ã«å–ã‚‰ã‚Œã¦ã„ãŸã®ã‹ã€‚\n",
    "    - https://www.kaggle.com/c/data-science-bowl-2019/discussion/126395#721312\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base on https://www.kaggle.com/artgor/quick-and-dirty-regression @artgor\n",
    "\n",
    "Please upvote the original kernel, thanks. ğŸ‘‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "random.seed(1029)\n",
    "np.random.seed(1029)\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import NuSVR, SVR\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "pd.options.display.precision = 15\n",
    "from collections import defaultdict\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cat\n",
    "import time\n",
    "from collections import Counter\n",
    "import datetime\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit, RepeatedStratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import linear_model\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from bayes_opt import BayesianOptimization\n",
    "import eli5\n",
    "import shap\n",
    "from IPython.display import HTML\n",
    "import json\n",
    "import altair as alt\n",
    "from category_encoders.ordinal import OrdinalEncoder\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from typing import List\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import gc\n",
    "from numba import jit\n",
    "\n",
    "from functools import partial\n",
    "import scipy as sp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "from sklearn import metrics\n",
    "from typing import Any\n",
    "from itertools import product\n",
    "pd.set_option('max_rows', 500)\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Constants(object):\n",
    "    eventIdCategorizer = [\"end of system-initiated instruction\", \"system-initiated instruction event\", \"help\", \"when an intro or outro movie starts to play\",\"The movie ended event\",\"when a video starts playing\", \"Correct\",\n",
    "    \"Incorrect\", \"interactive\", \"hover\", \"place\",\"drag\", \"click\", \"when the player finishes a round\", \"clicks on the play again\", \"the start of a round\",\"The start game event\", \"tutorial\",\"The exit game event\"]\n",
    "    game_category = {'Dino Drink':\"size\", 'Watering Hole (Activity)':\"size\", 'All Star Sorting':\"size\", 'Air Show':\"size\", 'Crystals Rule':\"length\", 'Bubble Bath':\"size\", 'Bottle Filler (Activity)':\"size\", 'Dino Dive':\"length\",'Happy Camel':\"weight\",\n",
    "    'Pan Balance':\"weight\", 'Egg Dropper (Activity)':\"weight\", 'Leaf Leader':\"weight\", 'Sandcastle Builder (Activity)':\"size\", 'Scrub-A-Dub':\"size\",'Chow Time':\"weight\",\n",
    "    'Fireworks (Activity)':\"length\",\n",
    "    'Flower Waterer (Activity)':\"length\",\n",
    "    'Bug Measurer (Activity)':\"length\",\n",
    "    'Chicken Balancer (Activity)':\"weight\"}\n",
    "    # \n",
    "    Assessment_category = {'Mushroom Sorter (Assessment)':\"size\", 'Bird Measurer (Assessment)':\"length\",\n",
    "        'Cauldron Filler (Assessment)':\"size\", 'Cart Balancer (Assessment)':\"weight\",\n",
    "        'Chest Sorter (Assessment)':\"size\"}\n",
    "    titles_dict = {'Dino Drink':{\"good\":'\"correct\":true',\"bad\":'\"correct\":false'},\n",
    "    'Watering Hole (Activity)':{\"good\":'\"filled\":true', \"bad\":'\"filled\":false'},\n",
    "    'All Star Sorting':{\"good\":'\"correct\":true',\"bad\":'\"correct\":false'},\n",
    "    'Air Show':{\"good\":'\"correct\":true',\"bad\":'\"correct\":false'},\n",
    "    'Crystals Rule':{\"good\":'\"correct\":true',\"bad\":'\"correct\":false'},\n",
    "    'Bubble Bath':{\"good\":'\"correct\":true',\"bad\":'\"correct\":false'},\n",
    "    'Bottle Filler (Activity)':{\"good\":[\"wowSoCool\",\"niceJob\",\"ohWow\"]},\n",
    "    'Dino Dive':{\"good\":'\"correct\":true',\"bad\":'\"correct\":false'},\n",
    "    'Happy Camel':{\"good\":'\"correct\":true',\"bad\":'\"correct\":false'},\n",
    "    'Pan Balance':{\"good\":'\"correct\":true',\"bad\":'\"correct\":false'},\n",
    "    'Egg Dropper (Activity)':{\"bad\":\"Buddy_EggsWentToOtherNest\", \"good\":\"Buddy_Incoming\"},\n",
    "    'Leaf Leader':{\"good\":'\"correct\":true',\"bad\":'\"correct\":false'},\n",
    "    'Sandcastle Builder (Activity)':{\"good\":[\"So cool!\", 'Great job! You did it!'],\"bad\":'need'},\n",
    "    'Scrub-A-Dub':{\"good\":'\"correct\":true',\"bad\":'\"correct\":false'},\n",
    "    'Chow Time':{\"good\":'\"correct\":true',\"bad\":'\"correct\":false'},\n",
    "    # not yet to be doneã¤ã‹å®Œå…¨ã«éŠã¶ã‚„ã¤\n",
    "    'Fireworks (Activity)':4000,\n",
    "    'Flower Waterer (Activity)':4000,\n",
    "    'Bug Measurer (Activity)':4000,\n",
    "    'Chicken Balancer (Activity)':4000}\n",
    "\n",
    "    categorical_features = [] #['session_title']\n",
    "    target_variable = 'accuracy_group'\n",
    "\n",
    "    def __init__(self, maps:dict, train_labels, Title_levels):\n",
    "        # import pdb; pdb.set_trace()\n",
    "        self._set_attr(maps_data=maps)\n",
    "        self.train_labels = train_labels\n",
    "        self.Title_levels = Title_levels\n",
    "        self.index_to_name(maps['activities_labels'])\n",
    "        fields = [\"titles_dict\", \"Assessment_category\",\"game_category\"]\n",
    "        for f_ in fields:\n",
    "            dict_ = getattr(Constants, f_)\n",
    "            indexed_dict = {}\n",
    "            for name, value in dict_.items():\n",
    "                indexed_dict[Constants.name2idx[name]] = value \n",
    "            setattr(Constants, f_+\"_indexed\" ,indexed_dict)\n",
    "        pass\n",
    "    @classmethod\n",
    "    def index_to_name(self, activities_labels):\n",
    "        name2idx = {}\n",
    "        for k, v in activities_labels.items():\n",
    "            name2idx[v] = k\n",
    "        self.name2idx = name2idx        \n",
    "    @classmethod\n",
    "    def _set_attr(self, maps_data:dict):\n",
    "        for m, dic in maps_data.items():\n",
    "            setattr(Constants, m ,dic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedRounder(object):\n",
    "    \"\"\"\n",
    "    An optimizer for rounding thresholds\n",
    "    to maximize Quadratic Weighted Kappa (QWK) score\n",
    "    # https://www.kaggle.com/naveenasaithambi/optimizedrounder-improved\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        \"\"\"\n",
    "        Get loss according to\n",
    "        using current coefficients\n",
    "        \n",
    "        :param coef: A list of coefficients that will be used for rounding\n",
    "        :param X: The raw predictions\n",
    "        :param y: The ground truth labels\n",
    "        \"\"\"\n",
    "        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n",
    "\n",
    "        return -qwk(y, X_p)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Optimize rounding thresholds\n",
    "        \n",
    "        :param X: The raw predictions\n",
    "        :param y: The ground truth labels\n",
    "        \"\"\"\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        initial_coef = [0.5, 1.5, 2.5]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        \"\"\"\n",
    "        Make predictions with specified thresholds\n",
    "        \n",
    "        :param X: The raw predictions\n",
    "        :param coef: A list of coefficients that will be used for rounding\n",
    "        \"\"\"\n",
    "        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n",
    "\n",
    "\n",
    "    def coefficients(self):\n",
    "        \"\"\"\n",
    "        Return the optimized coefficients\n",
    "        \"\"\"\n",
    "        return self.coef_['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "@jit\n",
    "def qwk(a1, a2):\n",
    "    \"\"\"\n",
    "    Source: https://www.kaggle.com/c/data-science-bowl-2019/discussion/114133#latest-660168\n",
    "\n",
    "    :param a1:\n",
    "    :param a2:\n",
    "    :param max_rat:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    max_rat = 3\n",
    "    a1 = np.asarray(a1, dtype=int)\n",
    "    a2 = np.asarray(a2, dtype=int)\n",
    "\n",
    "    hist1 = np.zeros((max_rat + 1, ))\n",
    "    hist2 = np.zeros((max_rat + 1, ))\n",
    "\n",
    "    o = 0\n",
    "    for k in range(a1.shape[0]):\n",
    "        i, j = a1[k], a2[k]\n",
    "        hist1[i] += 1\n",
    "        hist2[j] += 1\n",
    "        o +=  (i - j) * (i - j)\n",
    "\n",
    "    e = 0\n",
    "    for i in range(max_rat + 1):\n",
    "        for j in range(max_rat + 1):\n",
    "            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n",
    "\n",
    "    e = e / a1.shape[0]\n",
    "\n",
    "    return 1 - o / e\n",
    "\n",
    "\n",
    "def eval_qwk_lgb(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Fast cappa eval function for lgb.\n",
    "    \"\"\"\n",
    "\n",
    "    y_pred = y_pred.reshape(len(np.unique(y_true)), -1).argmax(axis=0)\n",
    "    return 'cappa', qwk(y_true, y_pred), True\n",
    "\n",
    "\n",
    "def eval_qwk_lgb_regr(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Fast cappa eval function for lgb.\n",
    "    \"\"\"\n",
    "    y_pred[y_pred <= 1.12232214] = 0\n",
    "    y_pred[np.where(np.logical_and(y_pred > 1.12232214, y_pred <= 1.73925866))] = 1\n",
    "    y_pred[np.where(np.logical_and(y_pred > 1.73925866, y_pred <= 2.22506454))] = 2\n",
    "    y_pred[y_pred > 2.22506454] = 3\n",
    "\n",
    "    # y_pred = y_pred.reshape(len(np.unique(y_true)), -1).argmax(axis=0)\n",
    "\n",
    "    return 'cappa', qwk(y_true, y_pred), True\n",
    "\n",
    "\n",
    "class LGBWrapper_regr(object):\n",
    "    \"\"\"\n",
    "    A wrapper for lightgbm model so that we will have a single api for various models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = lgb.LGBMRegressor()\n",
    "\n",
    "    def fit(self, X_train, y_train, X_valid=None, y_valid=None, X_holdout=None, y_holdout=None, params=None):\n",
    "        if params['objective'] == 'regression':\n",
    "            eval_metric = eval_qwk_lgb_regr\n",
    "        else:\n",
    "            eval_metric = 'auc'\n",
    "\n",
    "        eval_set = [(X_train, y_train)]\n",
    "        eval_names = ['train']\n",
    "        self.model = self.model.set_params(**params)\n",
    "\n",
    "        if X_valid is not None:\n",
    "            eval_set.append((X_valid, y_valid))\n",
    "            eval_names.append('valid')\n",
    "\n",
    "        if X_holdout is not None:\n",
    "            eval_set.append((X_holdout, y_holdout))\n",
    "            eval_names.append('holdout')\n",
    "\n",
    "        if 'cat_cols' in params.keys():\n",
    "            cat_cols = [col for col in params['cat_cols'] if col in X_train.columns]\n",
    "            if len(cat_cols) > 0:\n",
    "                categorical_columns = params['cat_cols']\n",
    "            else:\n",
    "                categorical_columns = 'auto'\n",
    "        else:\n",
    "            categorical_columns = 'auto'\n",
    "\n",
    "        self.model.fit(X=X_train, y=y_train,\n",
    "                       eval_set=eval_set, eval_names=eval_names, eval_metric=eval_metric,\n",
    "                       verbose=params['verbose'], early_stopping_rounds=params['early_stopping_rounds'],\n",
    "                       categorical_feature=categorical_columns)\n",
    "\n",
    "        self.best_score_ = self.model.best_score_\n",
    "        self.feature_importances_ = self.model.feature_importances_\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return self.model.predict(X_test, num_iteration=self.model.best_iteration_)\n",
    "\n",
    "    \n",
    "def eval_qwk_xgb(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Fast cappa eval function for xgb.\n",
    "    \"\"\"\n",
    "    # print('y_true', y_true)\n",
    "    # print('y_pred', y_pred)\n",
    "    y_true = y_true.get_label()\n",
    "    y_pred = y_pred.argmax(axis=1)\n",
    "    return 'cappa', -qwk(y_true, y_pred)\n",
    "\n",
    "class stackingMinxin(object):\n",
    "    \n",
    "    def fit_stacking(cls):\n",
    "        pass\n",
    "    def predict_stacking(cls):\n",
    "        pass\n",
    "    \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "class LGBWrapper(object):\n",
    "    \"\"\"\n",
    "    A wrapper for lightgbm model so that we will have a single api for various models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = lgb.LGBMClassifier()\n",
    "\n",
    "    def fit(self, X_train, y_train, X_valid=None, y_valid=None, X_holdout=None, y_holdout=None, params=None):\n",
    "\n",
    "        eval_set = [(X_train, y_train)]\n",
    "        eval_names = ['train']\n",
    "        self.model = self.model.set_params(**params)\n",
    "\n",
    "        if X_valid is not None:\n",
    "            eval_set.append((X_valid, y_valid))\n",
    "            eval_names.append('valid')\n",
    "\n",
    "        if X_holdout is not None:\n",
    "            eval_set.append((X_holdout, y_holdout))\n",
    "            eval_names.append('holdout')\n",
    "\n",
    "        if 'cat_cols' in params.keys():\n",
    "            cat_cols = [col for col in params['cat_cols'] if col in X_train.columns]\n",
    "            if len(cat_cols) > 0:\n",
    "                categorical_columns = params['cat_cols']\n",
    "            else:\n",
    "                categorical_columns = 'auto'\n",
    "        else:\n",
    "            categorical_columns = 'auto'\n",
    "\n",
    "        self.model.fit(X=X_train, y=y_train,\n",
    "                       eval_set=eval_set, eval_names=eval_names, eval_metric=eval_qwk_lgb,\n",
    "                       verbose=params['verbose'], early_stopping_rounds=params['early_stopping_rounds'],\n",
    "                       categorical_feature=categorical_columns)\n",
    "\n",
    "        self.best_score_ = self.model.best_score_\n",
    "        self.feature_importances_ = self.model.feature_importances_\n",
    "\n",
    "    def predict_proba(self, X_test):\n",
    "        if self.model.objective == 'binary':\n",
    "            return self.model.predict_proba(X_test, num_iteration=self.model.best_iteration_)[:, 1]\n",
    "        else:\n",
    "            return self.model.predict_proba(X_test, num_iteration=self.model.best_iteration_)\n",
    "\n",
    "\n",
    "class MainTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, convert_cyclical: bool = False, create_interactions: bool = False, n_interactions: int = 20):\n",
    "        \"\"\"\n",
    "        Main transformer for the data. Can be used for processing on the whole data.\n",
    "\n",
    "        :param convert_cyclical: convert cyclical features into continuous\n",
    "        :param create_interactions: create interactions between features\n",
    "        \"\"\"\n",
    "\n",
    "        self.convert_cyclical = convert_cyclical\n",
    "        self.create_interactions = create_interactions\n",
    "        self.feats_for_interaction = None\n",
    "        self.n_interactions = n_interactions\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        if self.create_interactions:\n",
    "            self.feats_for_interaction = [col for col in X.columns if 'sum' in col\n",
    "                                          or 'mean' in col or 'max' in col or 'std' in col\n",
    "                                          or 'attempt' in col]\n",
    "            self.feats_for_interaction1 = np.random.choice(self.feats_for_interaction, self.n_interactions)\n",
    "            self.feats_for_interaction2 = np.random.choice(self.feats_for_interaction, self.n_interactions)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        data = copy.deepcopy(X)\n",
    "        if self.create_interactions:\n",
    "            for col1 in self.feats_for_interaction1:\n",
    "                for col2 in self.feats_for_interaction2:\n",
    "                    data[f'{col1}_int_{col2}'] = data[col1] * data[col2]\n",
    "\n",
    "        if self.convert_cyclical:\n",
    "            data['timestampHour'] = np.sin(2 * np.pi * data['timestampHour'] / 23.0)\n",
    "            data['timestampMonth'] = np.sin(2 * np.pi * data['timestampMonth'] / 23.0)\n",
    "            data['timestampWeek'] = np.sin(2 * np.pi * data['timestampWeek'] / 23.0)\n",
    "            data['timestampMinute'] = np.sin(2 * np.pi * data['timestampMinute'] / 23.0)\n",
    "\n",
    "#         data['installation_session_count'] = data.groupby(['installation_id'])['Clip'].transform('count')\n",
    "#         data['installation_duration_mean'] = data.groupby(['installation_id'])['duration_mean'].transform('mean')\n",
    "#         data['installation_title_nunique'] = data.groupby(['installation_id'])['session_title'].transform('nunique')\n",
    "\n",
    "#         data['sum_event_code_count'] = data[['2000', '3010', '3110', '4070', '4090', '4030', '4035', '4021', '4020', '4010', '2080', '2083', '2040', '2020', '2030', '3021', '3121', '2050', '3020', '3120', '2060', '2070', '4031', '4025', '5000', '5010', '2081', '2025', '4022', '2035', '4040', '4100', '2010', '4110', '4045', '4095', '4220', '2075', '4230', '4235', '4080', '4050']].sum(axis=1)\n",
    "\n",
    "        # data['installation_event_code_count_mean'] = data.groupby(['installation_id'])['sum_event_code_count'].transform('mean')\n",
    "\n",
    "        return data\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        data = copy.deepcopy(X)\n",
    "        self.fit(data)\n",
    "        return self.transform(data)\n",
    "\n",
    "\n",
    "class FeatureTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, main_cat_features: list = None, num_cols: list = None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param main_cat_features:\n",
    "        :param num_cols:\n",
    "        \"\"\"\n",
    "        self.main_cat_features = main_cat_features\n",
    "        self.num_cols = num_cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "#         self.num_cols = [col for col in X.columns if 'sum' in col or 'mean' in col or 'max' in col or 'std' in col\n",
    "#                          or 'attempt' in col]\n",
    "        \n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        data = copy.deepcopy(X)\n",
    "#         for col in self.num_cols:\n",
    "#             data[f'{col}_to_mean'] = data[col] / data.groupby('installation_id')[col].transform('mean')\n",
    "#             data[f'{col}_to_std'] = data[col] / data.groupby('installation_id')[col].transform('std')\n",
    "\n",
    "        return data\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        data = copy.deepcopy(X)\n",
    "        self.fit(data)\n",
    "        return self.transform(data)\n",
    "    \n",
    "    \n",
    "class RegressorModel(object):\n",
    "    \"\"\"\n",
    "    A wrapper class for classification models.\n",
    "    It can be used for training and prediction.\n",
    "    Can plot feature importance and training progress (if relevant for model).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, columns: list = None, model_wrapper=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param original_columns:\n",
    "        :param model_wrapper:\n",
    "        \"\"\"\n",
    "        self.columns = columns\n",
    "        self.model_wrapper = model_wrapper\n",
    "        self.result_dict = {}\n",
    "        self.train_one_fold = False\n",
    "        self.preprocesser = None\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y,\n",
    "            X_holdout: pd.DataFrame = None, y_holdout=None,\n",
    "            folds=None,\n",
    "            params: dict = None,\n",
    "            eval_metric='rmse',\n",
    "            cols_to_drop: list = None,\n",
    "            preprocesser=None,\n",
    "            transformers: dict = None,\n",
    "            adversarial: bool = False,\n",
    "            plot: bool = True):\n",
    "        \"\"\"\n",
    "        Training the model.\n",
    "\n",
    "        :param X: training data\n",
    "        :param y: training target\n",
    "        :param X_holdout: holdout data\n",
    "        :param y_holdout: holdout target\n",
    "        :param folds: folds to split the data. If not defined, then model will be trained on the whole X\n",
    "        :param params: training parameters\n",
    "        :param eval_metric: metric for validataion\n",
    "        :param cols_to_drop: list of columns to drop (for example ID)\n",
    "        :param preprocesser: preprocesser class\n",
    "        :param transformers: transformer to use on folds\n",
    "        :param adversarial\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        if folds is None:\n",
    "            folds = KFold(n_splits=3, random_state=42)\n",
    "            self.train_one_fold = True\n",
    "\n",
    "        self.columns = X.columns if self.columns is None else self.columns\n",
    "        self.feature_importances = pd.DataFrame(columns=['feature', 'importance'])\n",
    "        self.trained_transformers = {k: [] for k in transformers}\n",
    "        self.transformers = transformers\n",
    "        self.models = []\n",
    "        self.folds_dict = {}\n",
    "        self.eval_metric = eval_metric\n",
    "        n_target = 1\n",
    "        self.oof = np.zeros((len(X), n_target))\n",
    "        self.n_target = n_target\n",
    "\n",
    "        X = X[self.columns]\n",
    "        if X_holdout is not None:\n",
    "            X_holdout = X_holdout[self.columns]\n",
    "\n",
    "        if preprocesser is not None:\n",
    "            self.preprocesser = preprocesser\n",
    "            self.preprocesser.fit(X, y)\n",
    "            X = self.preprocesser.transform(X, y)\n",
    "            self.columns = X.columns.tolist()\n",
    "            if X_holdout is not None:\n",
    "                X_holdout = self.preprocesser.transform(X_holdout)\n",
    "#         X.to_csv(f\"/kaggle/working/train_{datetime.datetime.now().strftime('%Y_%m_%d_%H%M%S')}.csv\", index=False)\n",
    "        for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y, X['installation_id'])):\n",
    "\n",
    "            if X_holdout is not None:\n",
    "                X_hold = X_holdout.copy()\n",
    "            else:\n",
    "                X_hold = None\n",
    "            self.folds_dict[fold_n] = {}\n",
    "            if params['verbose']:\n",
    "                print(f'Fold {fold_n + 1} started at {time.ctime()}')\n",
    "            self.folds_dict[fold_n] = {}\n",
    "\n",
    "            X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "            if self.train_one_fold:\n",
    "                X_train = X[self.original_columns]\n",
    "                y_train = y\n",
    "                X_valid = None\n",
    "                y_valid = None\n",
    "\n",
    "            datasets = {'X_train': X_train, 'X_valid': X_valid, 'X_holdout': X_hold, 'y_train': y_train}\n",
    "            X_train, X_valid, X_hold = self.transform_(datasets, cols_to_drop)\n",
    "\n",
    "            self.folds_dict[fold_n]['columns'] = X_train.columns.tolist()\n",
    "\n",
    "            model = copy.deepcopy(self.model_wrapper)\n",
    "\n",
    "            if adversarial:\n",
    "                X_new1 = X_train.copy()\n",
    "                if X_valid is not None:\n",
    "                    X_new2 = X_valid.copy()\n",
    "                elif X_holdout is not None:\n",
    "                    X_new2 = X_holdout.copy()\n",
    "                X_new = pd.concat([X_new1, X_new2], axis=0)\n",
    "                y_new = np.hstack((np.zeros((X_new1.shape[0])), np.ones((X_new2.shape[0]))))\n",
    "                X_train, X_valid, y_train, y_valid = train_test_split(X_new, y_new)\n",
    "\n",
    "            model.fit(X_train, y_train, X_valid, y_valid, X_hold, y_holdout, params=params)\n",
    "\n",
    "            self.folds_dict[fold_n]['scores'] = model.best_score_\n",
    "            if self.oof.shape[0] != len(X):\n",
    "                self.oof = np.zeros((X.shape[0], self.oof.shape[1]))\n",
    "            if not adversarial:\n",
    "                self.oof[valid_index] = model.predict(X_valid).reshape(-1, n_target)\n",
    "\n",
    "            fold_importance = pd.DataFrame(list(zip(X_train.columns, model.feature_importances_)),\n",
    "                                           columns=['feature', 'importance'])\n",
    "            self.feature_importances = self.feature_importances.append(fold_importance)\n",
    "            self.models.append(model)\n",
    "        \n",
    "#         pd.concat(self.feature_importances['importance']).to_csv(f\"/kaggle/working/importances_{datetime.datetime.now().strftime('%Y_%m_%d_%H%M%S')}.csv\")\n",
    "        self.feature_importances['importance'] = self.feature_importances['importance'].astype(int)\n",
    "\n",
    "        # if params['verbose']:\n",
    "        self.calc_scores_()\n",
    "\n",
    "        if plot:\n",
    "            # print(classification_report(y, self.oof.argmax(1)))\n",
    "            fig, ax = plt.subplots(figsize=(16, 12))\n",
    "            plt.subplot(2, 2, 1)\n",
    "            self.plot_feature_importance(top_n=20)\n",
    "            plt.subplot(2, 2, 2)\n",
    "            self.plot_metric()\n",
    "            plt.subplot(2, 2, 3)\n",
    "            plt.hist(y.values.reshape(-1, 1) - self.oof)\n",
    "            plt.title('Distribution of errors')\n",
    "            plt.subplot(2, 2, 4)\n",
    "            plt.hist(self.oof)\n",
    "            plt.title('Distribution of oof predictions');\n",
    "\n",
    "    def transform_(self, datasets, cols_to_drop):\n",
    "        for name, transformer in self.transformers.items():\n",
    "            transformer.fit(datasets['X_train'], datasets['y_train'])\n",
    "            datasets['X_train'] = transformer.transform(datasets['X_train'])\n",
    "            if datasets['X_valid'] is not None:\n",
    "                datasets['X_valid'] = transformer.transform(datasets['X_valid'])\n",
    "            if datasets['X_holdout'] is not None:\n",
    "                datasets['X_holdout'] = transformer.transform(datasets['X_holdout'])\n",
    "            self.trained_transformers[name].append(transformer)\n",
    "        if cols_to_drop is not None:\n",
    "            cols_to_drop = [col for col in cols_to_drop if col in datasets['X_train'].columns]\n",
    "\n",
    "            datasets['X_train'] = datasets['X_train'].drop(cols_to_drop, axis=1)\n",
    "            if datasets['X_valid'] is not None:\n",
    "                datasets['X_valid'] = datasets['X_valid'].drop(cols_to_drop, axis=1)\n",
    "            if datasets['X_holdout'] is not None:\n",
    "                datasets['X_holdout'] = datasets['X_holdout'].drop(cols_to_drop, axis=1)\n",
    "        self.cols_to_drop = cols_to_drop\n",
    "\n",
    "        return datasets['X_train'], datasets['X_valid'], datasets['X_holdout']\n",
    "\n",
    "    def calc_scores_(self):\n",
    "#         print()\n",
    "        datasets = [k for k, v in [v['scores'] for k, v in self.folds_dict.items()][0].items() if len(v) > 0]\n",
    "        self.scores = {}\n",
    "        for d in datasets:\n",
    "            scores = [v['scores'][d][self.eval_metric] for k, v in self.folds_dict.items()]\n",
    "#             print(f\"CV mean score on {d}: {np.mean(scores):.4f} +/- {np.std(scores):.4f} std.\")\n",
    "            self.scores[d] = np.mean(scores)\n",
    "\n",
    "    def predict(self, X_test, averaging: str = 'usual'):\n",
    "        \"\"\"\n",
    "        Make prediction\n",
    "\n",
    "        :param X_test:\n",
    "        :param averaging: method of averaging\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        full_prediction = np.zeros((X_test.shape[0], self.oof.shape[1]))\n",
    "        if self.preprocesser is not None:\n",
    "            X_test = self.preprocesser.transform(X_test)\n",
    "        for i in range(len(self.models)):\n",
    "            X_t = X_test.copy()\n",
    "            for name, transformers in self.trained_transformers.items():\n",
    "                X_t = transformers[i].transform(X_t)\n",
    "\n",
    "            if self.cols_to_drop is not None:\n",
    "                cols_to_drop = [col for col in self.cols_to_drop if col in X_t.columns]\n",
    "                X_t = X_t.drop(cols_to_drop, axis=1)\n",
    "            y_pred = self.models[i].predict(X_t[self.folds_dict[i]['columns']]).reshape(-1, full_prediction.shape[1])\n",
    "\n",
    "            # if case transformation changes the number of the rows\n",
    "            if full_prediction.shape[0] != len(y_pred):\n",
    "                full_prediction = np.zeros((y_pred.shape[0], self.oof.shape[1]))\n",
    "\n",
    "            if averaging == 'usual':\n",
    "                full_prediction += y_pred\n",
    "            elif averaging == 'rank':\n",
    "                full_prediction += pd.Series(y_pred).rank().values\n",
    "\n",
    "        return full_prediction / len(self.models)\n",
    "\n",
    "    def plot_feature_importance(self, drop_null_importance: bool = True, top_n: int = 10):\n",
    "        \"\"\"\n",
    "        Plot default feature importance.\n",
    "\n",
    "        :param drop_null_importance: drop columns with null feature importance\n",
    "        :param top_n: show top n columns\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        top_feats = self.get_top_features(drop_null_importance, top_n)\n",
    "        feature_importances = self.feature_importances.loc[self.feature_importances['feature'].isin(top_feats)]\n",
    "        feature_importances['feature'] = feature_importances['feature'].astype(str)\n",
    "        top_feats = [str(i) for i in top_feats]\n",
    "        sns.barplot(data=feature_importances, x='importance', y='feature', orient='h', order=top_feats)\n",
    "        plt.title('Feature importances')\n",
    "\n",
    "    def get_top_features(self, drop_null_importance: bool = True, top_n: int = 10):\n",
    "        \"\"\"\n",
    "        Get top features by importance.\n",
    "\n",
    "        :param drop_null_importance:\n",
    "        :param top_n:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        grouped_feats = self.feature_importances.groupby(['feature'])['importance'].mean()\n",
    "        if drop_null_importance:\n",
    "            grouped_feats = grouped_feats[grouped_feats != 0]\n",
    "        return list(grouped_feats.sort_values(ascending=False).index)[:top_n]\n",
    "\n",
    "    def plot_metric(self):\n",
    "        \"\"\"\n",
    "        Plot training progress.\n",
    "        Inspired by `plot_metric` from https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/plotting.html\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        full_evals_results = pd.DataFrame()\n",
    "        for model in self.models:\n",
    "            evals_result = pd.DataFrame()\n",
    "            for k in model.model.evals_result_.keys():\n",
    "                evals_result[k] = model.model.evals_result_[k][self.eval_metric]\n",
    "            evals_result = evals_result.reset_index().rename(columns={'index': 'iteration'})\n",
    "            full_evals_results = full_evals_results.append(evals_result)\n",
    "\n",
    "        full_evals_results = full_evals_results.melt(id_vars=['iteration']).rename(columns={'value': self.eval_metric,\n",
    "                                                                                            'variable': 'dataset'})\n",
    "        sns.lineplot(data=full_evals_results, x='iteration', y=self.eval_metric, hue='dataset')\n",
    "        plt.title('Training progress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class learningRoute(object):\n",
    "    ''''''\n",
    "    def __init__(self):\n",
    "        self.total_exp = None\n",
    "        self.title_crystalcaves = [\n",
    "            'Crystal Caves - Level 1',\n",
    "            'Chow Time',\n",
    "            'Balancing Act',\n",
    "            'Chicken Balancer (Activity)', \n",
    "            'Lifting Heavy Things', \n",
    "            'Crystal Caves - Level 2',\n",
    "            'Honey Cake',\n",
    "            'Happy Camel',\n",
    "            'Cart Balancer (Assessment)',\n",
    "            'Leaf Leader',\n",
    "            'Crystal Caves - Level 3',\n",
    "            'Heavy, Heavier, Heaviest',\n",
    "            'Pan Balance',\n",
    "            'Egg Dropper (Activity)',\n",
    "            'Chest Sorter (Assessment)'\n",
    "        ]\n",
    "        self.title_treetopcity = [\n",
    "            'Tree Top City - Level 1',\n",
    "            'Ordering Spheres',\n",
    "            'All Star Sorting',\n",
    "            'Costume Box',\n",
    "            'Fireworks (Activity)',\n",
    "            '12 Monkeys',\n",
    "            'Tree Top City - Level 2',\n",
    "            'Flower Waterer (Activity)',\n",
    "            \"Pirate's Tale\",\n",
    "            'Mushroom Sorter (Assessment)',\n",
    "            'Air Show',\n",
    "            'Treasure Map',\n",
    "            'Tree Top City - Level 3',\n",
    "            'Crystals Rule',\n",
    "            'Rulers',\n",
    "            'Bug Measurer (Activity)',\n",
    "            'Bird Measurer (Assessment)'\n",
    "        ]\n",
    "        self.title_magmapeak = [\n",
    "            'Magma Peak - Level 1',\n",
    "            'Sandcastle Builder (Activity)',\n",
    "            'Slop Problem',\n",
    "            'Scrub-A-Dub',\n",
    "            'Watering Hole (Activity)',\n",
    "            'Magma Peak - Level 2',\n",
    "            'Dino Drink',\n",
    "            'Bubble Bath',\n",
    "            'Bottle Filler (Activity)',\n",
    "            'Dino Dive',\n",
    "            'Cauldron Filler (Assessment)']\n",
    "\n",
    "    def _set_dict(self):\n",
    "        total_dic = dict()\n",
    "        setattr(self, 'total_exp', total_dic)\n",
    "        for k in ['title_crystalcaves', 'title_magmapeak', 'title_treetopcity']:\n",
    "            k_dic = {title_:0 for title_ in getattr(self, k)}\n",
    "            self.total_exp.update(k_dic.copy())\n",
    "        return self\n",
    "    def is_on_appropriate_route(self, world_text, Asessment_text):\n",
    "        titlesInWorld = getattr(self, 'title_' + world_text.lower())\n",
    "        _index = titlesInWorld.index(Asessment_text)\n",
    "        ans = 1\n",
    "        for title in titlesInWorld[0:_index]:\n",
    "            ans *= self.total_exp.get(title)\n",
    "        return ans\n",
    "    def record_experiences(self, title_text, world_text):\n",
    "        if world_text not in ['TREETOPCITY', 'CRYSTALCAVES', 'MAGMAPEAK']: return None\n",
    "        self.total_exp.update({title_text:1})\n",
    "\n",
    "\n",
    "class Title_levels(learningRoute):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.titlesLevel ={}\n",
    "        pass\n",
    "    def set_titlesLevel(self):\n",
    "        self._set_dict()\n",
    "        titlesLevel ={}\n",
    "        for k in ['title_crystalcaves', 'title_magmapeak', 'title_treetopcity']:\n",
    "            world_title_list = getattr(self, k)\n",
    "            level = 1\n",
    "            dic_ = {}\n",
    "            for title in world_title_list:\n",
    "                res = re.findall(string=title, pattern=r'Level +(\\d)')\n",
    "                if res and res[0] != str(level):\n",
    "                    level += 1\n",
    "                dic_[title] = k+'_level_'+str(level)\n",
    "            titlesLevel.update(dic_)\n",
    "            setattr(self,'titlesLevel' , titlesLevel)\n",
    "        pass\n",
    "    def __getitem__(self, index):\n",
    "        assert self.titlesLevel != {}, \"You have to implement 'set_titlesLevel'method beforehand\"\n",
    "        return self.titlesLevel.get(index)\n",
    "\n",
    "def countSession_eachlevels(count_dict, title_levels:Title_levels, title_text):\n",
    "    # assert set(count_dict.keys()) hogehoge\n",
    "    if title_levels[title_text]:\n",
    "        count_dict[title_levels[title_text]] += 1\n",
    "    return count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_dict_key(d, old_key, new_key, default_value=0):\n",
    "    d[new_key] = d.pop(old_key, default_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "titles_dict = {'Dino Drink':{\"good\":'\"correct\":true',\"bad\":'\"correct\":false'},\n",
    "'Watering Hole (Activity)':{\"good\":'\"filled\":true', \"bad\":'\"filled\":false'},\n",
    "'All Star Sorting':{\"good\":'\"correct\":true',\"bad\":'\"correct\":false'},\n",
    "'Air Show':{\"good\":'\"correct\":true',\"bad\":'\"correct\":false'},\n",
    "'Crystals Rule':{\"good\":'\"correct\":true',\"bad\":'\"correct\":false'},\n",
    "'Bubble Bath':{\"good\":'\"correct\":true',\"bad\":'\"correct\":false'},\n",
    "'Bottle Filler (Activity)':{\"good\":[\"wowSoCool\",\"niceJob\",\"ohWow\"]},\n",
    "'Dino Dive':{\"good\":'\"correct\":true',\"bad\":'\"correct\":false'},\n",
    "'Happy Camel':{\"good\":'\"correct\":true',\"bad\":'\"correct\":false'},\n",
    "'Pan Balance':{\"good\":'\"correct\":true',\"bad\":'\"correct\":false'},\n",
    "'Egg Dropper (Activity)':{\"bad\":\"Buddy_EggsWentToOtherNest\", \"good\":\"Buddy_Incoming\"},\n",
    "'Leaf Leader':{\"good\":'\"correct\":true',\"bad\":'\"correct\":false'},\n",
    "'Sandcastle Builder (Activity)':{\"good\":[\"So cool!\", 'Great job! You did it!'],\"bad\":'need'},\n",
    "'Scrub-A-Dub':{\"good\":'\"correct\":true',\"bad\":'\"correct\":false'},\n",
    "'Chow Time':{\"good\":'\"correct\":true',\"bad\":'\"correct\":false'},\n",
    "# not yet to be doneã¤ã‹å®Œå…¨ã«éŠã¶ã‚„ã¤\n",
    "'Fireworks (Activity)':4000,\n",
    "'Flower Waterer (Activity)':4000,\n",
    "'Bug Measurer (Activity)':4000,\n",
    " 'Chicken Balancer (Activity)':4000}# measureä½¿ã£ãŸã‹ã„ãªã‹ã§æ¸¬ã‚ã†ï¼\n",
    "\n",
    "def each_game_and_activity_score(titles_dict, session):\n",
    "    session_title = session['title'].iloc[0]\n",
    "    if isinstance(titles_dict[session_title], dict):\n",
    "        # find str\n",
    "        result = dict(good=0, bad=0)\n",
    "        good_and_bad_dict = titles_dict[session_title]\n",
    "        for key in [\"good\", \"bad\"]:\n",
    "            if good_and_bad_dict.get(key) and isinstance(good_and_bad_dict[key], str):\n",
    "                result[key] = session.event_data.str.contains(good_and_bad_dict[key]).sum()\n",
    "            elif good_and_bad_dict.get(key):\n",
    "                search_word = r\"|\".join(good_and_bad_dict[key])\n",
    "                counts = session.event_data.str.contains(search_word).sum()\n",
    "                # print(counts)\n",
    "                result[key] = counts    \n",
    "        summand = result[\"good\"] + result[\"bad\"]\n",
    "        return result[\"good\"]/summand if summand >0 else -1 \n",
    "    elif titles_dict[session_title] == 4000:\n",
    "        # just a pure activity so that I want to count how many users tap or enjoy the one. \n",
    "        return session.event_data.str.contains(r'\"event_code\":4\\d{3}').sum()\n",
    "def read_data():\n",
    "    print('Reading train.csv file....')\n",
    "    train = pd.read_csv('/kaggle/input/data-science-bowl-2019/train.csv')\n",
    "    print('Training.csv file have {} rows and {} columns'.format(train.shape[0], train.shape[1]))\n",
    "\n",
    "    print('Reading test.csv file....')\n",
    "    test = pd.read_csv('/kaggle/input/data-science-bowl-2019/test.csv')\n",
    "    print('Test.csv file have {} rows and {} columns'.format(test.shape[0], test.shape[1]))\n",
    "\n",
    "    print('Reading train_labels.csv file....')\n",
    "    train_labels = pd.read_csv('/kaggle/input/data-science-bowl-2019/train_labels.csv')\n",
    "    print('Train_labels.csv file have {} rows and {} columns'.format(train_labels.shape[0], train_labels.shape[1]))\n",
    "\n",
    "    print('Reading specs.csv file....')\n",
    "    specs = pd.read_csv('/kaggle/input/data-science-bowl-2019/specs.csv')\n",
    "    print('Specs.csv file have {} rows and {} columns'.format(specs.shape[0], specs.shape[1]))\n",
    "\n",
    "    print('Reading sample_submission.csv file....')\n",
    "    sample_submission = pd.read_csv('/kaggle/input/data-science-bowl-2019/sample_submission.csv')\n",
    "    print('Sample_submission.csv file have {} rows and {} columns'.format(sample_submission.shape[0], sample_submission.shape[1]))\n",
    "    return train, test, train_labels, specs, sample_submission\n",
    "\n",
    "def encode_title(train, test, train_labels):\n",
    "    # encode title\n",
    "    train['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), train['title'], train['event_code']))\n",
    "    test['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code']))\n",
    "    all_title_event_code = list(set(train[\"title_event_code\"].unique()).union(test[\"title_event_code\"].unique()))\n",
    "    \n",
    "    # make a list with all the unique 'titles' from the train and test set\n",
    "    list_of_user_activities = list(set(train['title'].unique()).union(set(test['title'].unique())))\n",
    "    \n",
    "    # make a list with all the unique 'event_code' from the train and test set\n",
    "    list_of_event_code = list(set(train['event_code'].unique()).union(set(test['event_code'].unique())))\n",
    "    list_of_event_id = list(set(train['event_id'].unique()).union(set(test['event_id'].unique())))\n",
    "    \n",
    "    # make a list with all the unique worlds from the train and test set\n",
    "    list_of_worlds = list(set(train['world'].unique()).union(set(test['world'].unique())))\n",
    "    # create a dictionary numerating the titles\n",
    "    activities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n",
    "    activities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n",
    "    \n",
    "    activities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n",
    "    activities_world_labels = dict(zip(np.arange(len(list_of_worlds)), list_of_worlds))\n",
    "    assess_titles = list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(set(test[test['type'] == 'Assessment']['title'].value_counts().index)))\n",
    "\n",
    "\n",
    "    # replace the text titles with the number titles from the dict\n",
    "    train['title'] = train['title'].map(activities_map)\n",
    "    test['title'] = test['title'].map(activities_map)\n",
    "    train['world'] = train['world'].map(activities_world)\n",
    "    test['world'] = test['world'].map(activities_world)\n",
    "    train_labels['title'] = train_labels['title'].map(activities_map)\n",
    "    win_code = dict(zip(list_of_user_activities, (4100*np.ones(len(list_of_user_activities))).astype('int')))\n",
    "    assess_indices = list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(set(test[test['type'] == 'Assessment']['title'].value_counts().index)))\n",
    "    # then, it set one element, the 'Bird Measurer (Assessment)' as 4110, 10 more than the rest\n",
    "    win_code['Bird Measurer (Assessment)'] = 4110\n",
    "    # convert text into datetime\n",
    "    train['timestamp'] = pd.to_datetime(train['timestamp'])\n",
    "    test['timestamp'] = pd.to_datetime(test['timestamp'])\n",
    "\n",
    "    maps_data = {\"activities_map\": activities_map, 'assess_titles': assess_titles,\"list_of_user_activities\":list_of_user_activities,\n",
    "                'list_of_event_code':list_of_event_code, 'list_of_event_id':list_of_event_id, 'all_title_event_code': all_title_event_code,\n",
    "                'activities_labels':activities_labels, 'activities_world':activities_world, \n",
    "                'activities_world_labels':activities_world_labels,\n",
    "                 \"win_code\":win_code, 'assess_indices':assess_indices}\n",
    "    return train, test, train_labels, maps_data \n",
    "\n",
    "def count_to_share(count_dict, denominator=None):\n",
    "    \"\"\"2ndprocessinã«ã¦total_count by installation_idã€€ã§å‰²ã‚‹\"\"\"\n",
    "    if denominator is None:\n",
    "        # we have to set denominator as total of count_dict\n",
    "        denominator = sum(count_dict.values())\n",
    "    for k, v in count_dict.items():\n",
    "        count_dict[k] = v/denominator if denominator !=0 else 0\n",
    "    return count_dict\n",
    "\n",
    "class Features:\n",
    "    pass\n",
    "class Games(Features):\n",
    "    def __init__(self):\n",
    "        self.mean_game_duration = 0\n",
    "        self.mean_game_round = 0\n",
    "        self.mean_game_level = 0\n",
    "        self.Game_mean_event_count = 0\n",
    "        self.accumulated_game_miss = 0\n",
    "        self.game_miss_mean_count = 0\n",
    "    \n",
    "\n",
    "def cnt_miss(df):\n",
    "    cnt = 0\n",
    "    for e in range(len(df)):\n",
    "        x = df['event_data'].iloc[e]\n",
    "        y = json.loads(x)['misses']\n",
    "        cnt += y\n",
    "    return cnt\n",
    "\n",
    "def game_features(games:Games, session):\n",
    "\n",
    "    games.Game_mean_event_count = (games.Game_mean_event_count + session['event_count'].iloc[-1])/2.0\n",
    "    game_s = session[session.event_code == 2030]   \n",
    "    misses_cnt = cnt_miss(game_s)\n",
    "    games.accumulated_game_miss += misses_cnt\n",
    "    games.game_miss_mean_count += (misses_cnt + games.game_miss_mean_count)/2.0\n",
    "    try:\n",
    "        game_round = json.loads(session['event_data'].iloc[-1])[\"round\"]\n",
    "        games.mean_game_round =  (games.mean_game_round + game_round)/2.0\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        game_duration = json.loads(session['event_data'].iloc[-1])[\"duration\"]\n",
    "        games.mean_game_duration = (games.mean_game_duration + game_duration) /2.0\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        game_level = json.loads(session['event_data'].iloc[-1])[\"level\"]\n",
    "        games.mean_game_level = (games.mean_game_level + game_level) /2.0\n",
    "    except:\n",
    "        pass\n",
    "    return games\n",
    "\n",
    "# constantså°å…¥ã«ã¤ãã€titles_dictæ¶ˆã™\n",
    "def get_data(installation_id, user_sample, constants:Constants, test_set=False):\n",
    "    '''\n",
    "    The user_sample is a DataFrame from train or test where the only one \n",
    "    installation_id is filtered\n",
    "    And the test_set parameter is related with the labels processing, that is only requered\n",
    "    if test_set=False\n",
    "    '''\n",
    "    \n",
    "    # Constants and parameters declaration\n",
    "    last_activity = 0\n",
    "    \n",
    "    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n",
    "    \n",
    "    # new features: time spent in each activity\n",
    "    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n",
    "    all_assessments = []\n",
    "    accumulated_accuracy_group = 0\n",
    "    accumulated_accuracy = 0\n",
    "    accumulated_correct_attempts = 0 \n",
    "    accumulated_uncorrect_attempts = 0\n",
    "    accumulated_actions = 0\n",
    "    counter = 0\n",
    "    time_first_activity = float(user_sample['timestamp'].values[0])\n",
    "    durations = []\n",
    "    # last_accuracy_titleå…¥ã‚ŒãŸã£ã‘ï¼Ÿ\n",
    "    # last_accuracy_title = {'acc_' + title:0 for title in constants.assess_titles}\n",
    "\n",
    "    event_code_count: Dict[str, int] = {ev: 0 for ev in constants.list_of_event_code}\n",
    "    event_id_count: Dict[str, int] = {eve: 0 for eve in constants.list_of_event_id}\n",
    "    title_count: Dict[str, int] = {eve: 0 for eve in constants.list_of_user_activities}\n",
    "    \n",
    "    title_event_code_count: Dict[str, int] = {t_eve: 0 for t_eve in constants.all_title_event_code}\n",
    "    # calculate the last score of each activity\n",
    "    gameActivityScores = {'score_title_' + str(ga_title): 0 for ga_title in constants.titles_dict.keys()}\n",
    "\n",
    "    level_counts = {'title_treetopcity_level_1':0, 'title_treetopcity_level_2':0, 'title_treetopcity_level_3':0,\n",
    "                    'title_magmapeak_level_1':0, 'title_magmapeak_level_2':0, 'title_crystalcaves_level_1':0,\n",
    "                    'title_crystalcaves_level_2':0, 'title_crystalcaves_level_3':0, \n",
    "                    }\n",
    "    # game related\n",
    "    games = Games()\n",
    "    # Activity related\n",
    "    Activity_mean_event_count = 0\n",
    "    \n",
    "    # after submit 1/27\n",
    "    # is_first_assess\n",
    "    is_first_assessment = 1\n",
    "\n",
    "    # length, size, weightã«é–¢é€£ã™ã‚‹activityæ•°ã‚’å½¢çŠ¶ã™ã‚‹ã€‚\n",
    "    # activityæ•°ã˜ã‚ƒãªã„ã‚‚ã£ã¨ç›®çš„ã«æ²¿ã£ãŸæŒ‡æ¨™ã‚’ç”Ÿæˆã—ãŸã„ã€‚\n",
    "    activity_type = {'length':0, 'size':0, 'weight':0}\n",
    "    check_routes = learningRoute()._set_dict()\n",
    "\n",
    "    # itarates through each session of one instalation_id\n",
    "    for i, session in user_sample.groupby('game_session', sort=False):\n",
    "        # i = game_session_id\n",
    "        # session is a DataFrame that contain only one game_session\n",
    "        # get some sessions information\n",
    "        session_type = session['type'].iloc[0]\n",
    "        session_title = session['title'].iloc[0]\n",
    "        session_title_text = constants.activities_labels[session_title]         \n",
    "        session_world = session['world'].iloc[0]\n",
    "        check_routes.record_experiences(title_text=session_title_text,\n",
    "                           world_text=constants.activities_world_labels[session_world])\n",
    "        level_counts = countSession_eachlevels(level_counts, constants.Title_levels, session_title_text)\n",
    "        # if gameActivityScores.get('score_title_' + session_title) is not None:\n",
    "        if constants.game_category.get(session_title_text) is not None:\n",
    "            # import pdb;pdb.set_trace()\n",
    "            # score_ = each_game_and_activity_score(titles_dict, session)\n",
    "            # print(score_)\n",
    "            # gameActivityScores['score_title_' + session_title] = score_\n",
    "            # ã¨ã‚Šã‚ãˆãšã‚¤ãƒ™ãƒ³ãƒˆã‚«ã‚¦ãƒ³ãƒˆã§ã€‚scoreã§ã‚‚ã€duration_timeã§ã‚‚ã„ã„ãªã€‚\n",
    "            activity_type[constants.game_category[session_title_text]] += session.game_time.max()\n",
    "\n",
    "        if session_type==\"Activity\":\n",
    "            Activity_mean_event_count = (Activity_mean_event_count + session['event_count'].iloc[-1])/2.0        \n",
    "        if session_type==\"Game\":\n",
    "            games = game_features(games, session)\n",
    "        # for each assessment, and only this kind off session, the features below are processed\n",
    "        # and a register are generated\n",
    "        if (session_type == 'Assessment') & (test_set or len(session)>1):\n",
    "            # on_route_values = check_routes.is_on_appropriate_route(world_text=constants.activities_world_labels[session_world],\n",
    "            #                                      Asessment_text=session_title_text)\n",
    "            \n",
    "            # import pdb; pdb.set_trace()\n",
    "            # search for event_code 4100, that represents the assessments trial\n",
    "            all_attempts = session.query(f'event_code == {constants.win_code[constants.activities_labels[session_title]]}')\n",
    "            # then, check the numbers of wins and the number of losses\n",
    "            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n",
    "            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n",
    "            # copy a dict to use as feature template, it's initialized with some itens: \n",
    "            \n",
    "            # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n",
    "            features = count_to_share(user_activities_count).copy()\n",
    "            \n",
    "            features.update(count_to_share(title_count).copy())\n",
    "            features.update(count_to_share(title_event_code_count).copy())\n",
    "            \n",
    "            features.update(gameActivityScores.copy())\n",
    "            # added game features\n",
    "            features.update(games.__dict__.copy())\n",
    "            # added an activity feature\n",
    "            features['Activity_mean_event_count'] = Activity_mean_event_count\n",
    "            \n",
    "            # after submit\n",
    "            features.update({\"is_first_assessment\":is_first_assessment}.copy())\n",
    "            if is_first_assessment == 1:\n",
    "                is_first_assessment = 0\n",
    "\n",
    "\n",
    "            features.update(activity_type.copy())\n",
    "            # features.update({\"on_route\":on_route_values})\n",
    "            # features.update(last_accuracy_title.copy())\n",
    "            features.update(count_to_share(event_code_count).copy())\n",
    "            # features.update(event_id_count.copy())\n",
    "\n",
    "            features.update(count_to_share(level_counts.copy()))\n",
    "            # get installation_id for aggregated features\n",
    "            features['installation_id'] = session['installation_id'].iloc[-1]\n",
    "\n",
    "            # add title as feature, remembering that title represents the name of the game\n",
    "            for assess in constants.assess_indices:\n",
    "                one_hot_session_title = 0\n",
    "                if assess == session['title'].iloc[0]:\n",
    "                    one_hot_session_title = 1\n",
    "                features[f'session_title_{assess}'] = one_hot_session_title\n",
    "            \n",
    "            \n",
    "\n",
    "            # the 4 lines below add the feature of the history of the trials of this player\n",
    "            # this is based on the all time attempts so far, at the moment of this assessment\n",
    "            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n",
    "            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n",
    "            accumulated_correct_attempts += true_attempts \n",
    "            accumulated_uncorrect_attempts += false_attempts\n",
    "            # the time spent in the app so far\n",
    "            if durations == []:\n",
    "                features['duration_mean'] = 0\n",
    "            else:\n",
    "                features['duration_mean'] = np.mean(durations)\n",
    "            durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n",
    "            # the accurace is the all time wins divided by the all time attempts\n",
    "            features['accumulated_accuracy'] = accumulated_accuracy/counter if counter > 0 else 0\n",
    "            accuracy = true_attempts/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n",
    "            accumulated_accuracy += accuracy\n",
    "            # last_accuracy_title['acc_' + str(session_title)] = accuracy\n",
    "            # a feature of the current accuracy categorized\n",
    "            # it is a counter of how many times this player was in each accuracy group\n",
    "            if accuracy == 0:\n",
    "                features['accuracy_group'] = 0\n",
    "            elif accuracy == 1:\n",
    "                features['accuracy_group'] = 3\n",
    "            elif accuracy == 0.5:\n",
    "                features['accuracy_group'] = 2\n",
    "            else:\n",
    "                features['accuracy_group'] = 1\n",
    "            features.update(accuracy_groups)\n",
    "            accuracy_groups[features['accuracy_group']] += 1\n",
    "            # mean of the all accuracy groups of this player\n",
    "            features['accumulated_accuracy_group'] = accumulated_accuracy_group/counter if counter > 0 else 0\n",
    "            accumulated_accuracy_group += features['accuracy_group']\n",
    "            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n",
    "            features['accumulated_actions'] = accumulated_actions\n",
    "            \n",
    "            # there are some conditions to allow this features to be inserted in the datasets\n",
    "            # if it's a test set, all sessions belong to the final dataset\n",
    "            # it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title]}')\n",
    "            # that means, must exist an event_code 4100 or 4110\n",
    "            if test_set:\n",
    "                all_assessments.append(features)\n",
    "            elif true_attempts+false_attempts > 0:\n",
    "                features['num_incorrect'] = false_attempts\n",
    "                features['num_correct'] = true_attempts\n",
    "                # print(false_attempts)\n",
    "        # constants.train_labels.query(f'installation_id == \"{installation_id}\" and game_session == \"{i}\"')['num_incorrect'].iloc[0]\n",
    "                all_assessments.append(features)\n",
    "                \n",
    "            counter += 1\n",
    "        \n",
    "        # this piece counts how many actions was made in each event_code so far\n",
    "        def update_counters(counter: dict, col: str):\n",
    "                num_of_session_count = Counter(session[col])\n",
    "                for k in num_of_session_count.keys():\n",
    "                    x = k\n",
    "                    if col == 'title':\n",
    "                        x = constants.activities_labels[k]\n",
    "                    counter[x] += num_of_session_count[k]\n",
    "                return counter\n",
    "            \n",
    "        event_code_count = update_counters(event_code_count, \"event_code\")\n",
    "        # event_id_count = update_counters(event_id_count, \"event_id\")\n",
    "        title_count = update_counters(title_count, 'title')\n",
    "        title_event_code_count = update_counters(title_event_code_count, 'title_event_code')\n",
    "\n",
    "        # counts how many actions the player has done so far, used in the feature of the same name\n",
    "        accumulated_actions += len(session)\n",
    "        if last_activity != session_type:\n",
    "            user_activities_count[session_type] += 1\n",
    "            last_activitiy = session_type \n",
    "                        \n",
    "    # if it't the test_set, only the last assessment must be predicted, the previous are scraped\n",
    "    if test_set:\n",
    "        return all_assessments[-1]\n",
    "    # in the train_set, all assessments goes to the dataset\n",
    "    return all_assessments\n",
    "\n",
    "def get_train_and_test(train, test, constants):\n",
    "    compiled_train = []\n",
    "    compiled_test = []\n",
    "    for i, (ins_id, user_sample) in tqdm(enumerate(train.groupby('installation_id', sort = False)), total = 17000):\n",
    "        compiled_train += get_data(ins_id, user_sample, constants)\n",
    "    for ins_id, user_sample in tqdm(test.groupby('installation_id', sort = False), total = 1000):\n",
    "        compiled_train += get_data(ins_id, user_sample, constants)\n",
    "        test_data = get_data(ins_id, user_sample, constants,test_set = True)\n",
    "        compiled_test.append(test_data)\n",
    "    reduce_train = pd.DataFrame(compiled_train)\n",
    "    reduce_test = pd.DataFrame(compiled_test)\n",
    "    categoricals = []#['session_title']\n",
    "    return reduce_train, reduce_test, categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encoder(reduce_train, reduce_test, category_cols):\n",
    "    for col in category_cols:\n",
    "        one_hot_vectors = pd.get_dummies(pd.concat([reduce_train[col],reduce_test[col]], axis=0))\n",
    "        one_hot_vectors.columns = [col+'_'+str(i) for i in one_hot_vectors.columns]\n",
    "        reduce_train.drop(columns=col, inplace=True);reduce_test.drop(columns=col, inplace=True)\n",
    "        reduce_train = pd.concat([reduce_train, one_hot_vectors[:len(reduce_train)]], axis=1)\n",
    "        reduce_test = pd.concat([reduce_test, one_hot_vectors[len(reduce_train):]], axis=1)\n",
    "    return reduce_train, reduce_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "train, test, train_labels, specs, sample_submission = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get usefull dict with maping encode\n",
    "train, test, train_labels, maps_data = encode_title(train, test, train_labels)\n",
    "t_levels = Title_levels()\n",
    "t_levels.set_titlesLevel()\n",
    "constants = Constants(maps_data, train_labels, t_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tranform function to get the train and test set\n",
    "reduce_train, reduce_test, categoricals = get_train_and_test(train, test, constants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess(reduce_train, reduce_test):\n",
    "#     for df in [reduce_train, reduce_test]:\n",
    "#         df['installation_session_count'] = df.groupby(['installation_id'])['Clip'].transform('count')\n",
    "#         df['installation_duration_mean'] = df.groupby(['installation_id'])['duration_mean'].transform('mean')\n",
    "#         #df['installation_duration_std'] = df.groupby(['installation_id'])['duration_mean'].transform('std')\n",
    "#         df['installation_title_nunique'] = df.groupby(['installation_id'])['session_title'].transform('nunique')\n",
    "        \n",
    "#         df['sum_event_code_count'] = df[[2050, 4100, 4230, 5000, 4235, 2060, 4110, 5010, 2070, 2075, 2080, 2081, 2083, 3110, 4010, 3120, 3121, 4020, 4021, \n",
    "#                                         4022, 4025, 4030, 4031, 3010, 4035, 4040, 3020, 3021, 4045, 2000, 4050, 2010, 2020, 4070, 2025, 2030, 4080, 2035, \n",
    "#                                         2040, 4090, 4220, 4095]].sum(axis = 1)\n",
    "        \n",
    "#         df['installation_event_code_count_mean'] = df.groupby(['installation_id'])['sum_event_code_count'].transform('mean')\n",
    "#         #df['installation_event_code_count_std'] = df.groupby(['installation_id'])['sum_event_code_count'].transform('std')\n",
    "        \n",
    "#     features = reduce_train.loc[(reduce_train.sum(axis=1) != 0), (reduce_train.sum(axis=0) != 0)].columns # delete useless columns\n",
    "#     features = [x for x in features if x not in ['accuracy_group', 'installation_id']] + ['acc_' + title for title in assess_titles]\n",
    "   \n",
    "#     return reduce_train, reduce_test, features\n",
    "def preprocess(reduce_train, reduce_test, constants:Constants):\n",
    "    for df in [reduce_train, reduce_test]:\n",
    "        # df['installation_session_count'] = df.groupby(['installation_id'])['Clip'].transform('count')\n",
    "        df['installation_duration_mean'] = df.groupby(['installation_id'])['duration_mean'].transform('mean')\n",
    "        #df['installation_duration_std'] = df.groupby(['installation_id'])['duration_mean'].transform('std')\n",
    "        # df['installation_title_nunique'] = df.groupby(['installation_id'])['session_title'].transform('nunique')\n",
    "        event_codes = [2050, 4100, 4230, 5000, 4235, 2060, 4110, 5010, 2070, 2075, 2080, 2081, 2083, 3110, 4010, 3120, 3121, 4020, 4021, 4022, 4025, 4030, 4031, 3010, 4035, 4040, 3020, 3021, 4045, 2000, 4050, 2010, 2020, 4070, 2025, 2030, 4080, 2035, 2040, 4090, 4220, 4095]\n",
    "        event_codes_str = [str(ec) for ec in event_codes]\n",
    "        # df['sum_event_code_count'] = df[[str(ec) for ec in event_codes ]].sum(axis = 1)\n",
    "        \n",
    "        # df['installation_event_code_count_mean'] = df.groupby(['installation_id'])['sum_event_code_count'].transform('mean')\n",
    "        #df['installation_event_code_count_std'] = df.groupby(['installation_id'])['sum_event_code_count'].transform('std')\n",
    "        # try:\n",
    "        #     df.drop(columns=event_codes, inplace=True)\n",
    "        # except KeyError:\n",
    "        #     df.drop(columns=event_codes_str, inplace=True)\n",
    "        \n",
    "        # reduce_train = target_encoder(reduce_train, constants.target_variable, constants.categorical_features)\n",
    "\n",
    "    # for stacking by Lasso.   \n",
    "    # reduce_train, reduce_test = onehot_encoder(reduce_train, reduce_test, constants.categoricals)\n",
    "    features = reduce_train.loc[(reduce_train.sum(axis=1) != 0), (reduce_train.sum(axis=0) != 0)].columns # delete useless columns\n",
    "    features = [x for x in features if x not in ['accuracy_group', 'installation_id']] + ['acc_' + title for title in constants.assess_titles]\n",
    "    return reduce_train, reduce_test, features\n",
    "\n",
    "\n",
    "# call feature engineering function\n",
    "reduce_train, reduce_test, features = preprocess(reduce_train, reduce_test, constants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = reduce_train['accuracy_group']\n",
    "\n",
    "cols_to_drop = ['game_session', 'installation_id', 'timestamp', \n",
    "                'accuracy_group', 'timestampDate', \n",
    "                'num_incorrect','num_correct']\n",
    "\n",
    "n_fold = 5\n",
    "folds = GroupKFold(n_splits=n_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LGB_bayesian(max_depth,\n",
    "                 lambda_l1,\n",
    "                 lambda_l2,\n",
    "                 bagging_fraction,\n",
    "                 bagging_freq,\n",
    "                 colsample_bytree,\n",
    "                 learning_rate):\n",
    "    \n",
    "    params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'metric': 'rmse',\n",
    "        'objective': 'regression',\n",
    "        'eval_metric': 'cappa',\n",
    "        'n_jobs': -1,\n",
    "        'seed': 42,\n",
    "        'early_stopping_rounds': 100,\n",
    "        'n_estimators': 2000,\n",
    "        'learning_rate': learning_rate,\n",
    "        'max_depth': int(max_depth),\n",
    "        'lambda_l1': lambda_l1,\n",
    "        'lambda_l2': lambda_l2,\n",
    "        'bagging_fraction': bagging_fraction,\n",
    "        'bagging_freq': int(bagging_freq),\n",
    "        'colsample_bytree': colsample_bytree,\n",
    "        'verbose': 0\n",
    "    }\n",
    "    \n",
    "    mt = MainTransformer()\n",
    "    ft = FeatureTransformer()\n",
    "    transformers = {'ft': ft}\n",
    "    model = RegressorModel(model_wrapper=LGBWrapper_regr())\n",
    "    model.fit(X=reduce_train, \n",
    "              y=y, \n",
    "              folds=folds, \n",
    "              params=params, \n",
    "              preprocesser=mt, \n",
    "              transformers=transformers,\n",
    "              eval_metric='cappa', \n",
    "              cols_to_drop=cols_to_drop,\n",
    "              plot=False)\n",
    "    \n",
    "    return model.scores['valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_points = 16\n",
    "n_iter = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds_LGB = {\n",
    "    'max_depth': (8, 11),\n",
    "    'lambda_l1': (0, 5),\n",
    "    'lambda_l2': (0, 5),\n",
    "    'bagging_fraction': (0.4, 0.6),\n",
    "    'bagging_freq': (1, 10),\n",
    "    'colsample_bytree': (0.4, 0.6),\n",
    "    'learning_rate': (0.05, 0.1)\n",
    "}\n",
    "\n",
    "LGB_BO = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=1029)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore')\n",
    "    LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'metric': 'rmse',\n",
    "    'objective': 'regression',\n",
    "    'eval_metric': 'cappa',\n",
    "    'n_jobs': -1,\n",
    "    'seed': 42,\n",
    "    'early_stopping_rounds': 100,\n",
    "    'n_estimators': 2000,\n",
    "    'learning_rate': LGB_BO.max['params']['learning_rate'],\n",
    "    'max_depth': int(LGB_BO.max['params']['max_depth']),\n",
    "    'lambda_l1': LGB_BO.max['params']['lambda_l1'],\n",
    "    'lambda_l2': LGB_BO.max['params']['lambda_l2'],\n",
    "    'bagging_fraction': LGB_BO.max['params']['bagging_fraction'],\n",
    "    'bagging_freq': int(LGB_BO.max['params']['bagging_freq']),\n",
    "    'colsample_bytree': LGB_BO.max['params']['colsample_bytree'],\n",
    "    'verbose': 100\n",
    "}\n",
    "\n",
    "mt = MainTransformer()\n",
    "ft = FeatureTransformer()\n",
    "transformers = {'ft': ft}\n",
    "regressor_model = RegressorModel(model_wrapper=LGBWrapper_regr())\n",
    "regressor_model.fit(X=reduce_train, \n",
    "                    y=y, \n",
    "                    folds=folds, \n",
    "                    params=params, \n",
    "                    preprocesser=mt, \n",
    "                    transformers=transformers,\n",
    "                    eval_metric='cappa', \n",
    "                    cols_to_drop=cols_to_drop)\n",
    "\n",
    "preds_1 = regressor_model.predict(reduce_test)\n",
    "w_1 = LGB_BO.max['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del bounds_LGB, LGB_BO, params, mt, ft, transformers, regressor_model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds_LGB = {\n",
    "    'max_depth': (11, 14),\n",
    "    'lambda_l1': (0, 10),\n",
    "    'lambda_l2': (0, 10),\n",
    "    'bagging_fraction': (0.7, 1),\n",
    "    'bagging_freq': (1, 10),\n",
    "    'colsample_bytree': (0.7, 1),\n",
    "    'learning_rate': (0.08, 0.2)\n",
    "}\n",
    "\n",
    "LGB_BO = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=1030)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore')\n",
    "    LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'metric': 'rmse',\n",
    "    'objective': 'regression',\n",
    "    'eval_metric': 'cappa',\n",
    "    'n_jobs': -1,\n",
    "    'seed': 42,\n",
    "    'early_stopping_rounds': 100,\n",
    "    'n_estimators': 2000,\n",
    "    'learning_rate': LGB_BO.max['params']['learning_rate'],\n",
    "    'max_depth': int(LGB_BO.max['params']['max_depth']),\n",
    "    'lambda_l1': LGB_BO.max['params']['lambda_l1'],\n",
    "    'lambda_l2': LGB_BO.max['params']['lambda_l2'],\n",
    "    'bagging_fraction': LGB_BO.max['params']['bagging_fraction'],\n",
    "    'bagging_freq': int(LGB_BO.max['params']['bagging_freq']),\n",
    "    'colsample_bytree': LGB_BO.max['params']['colsample_bytree'],\n",
    "    'verbose': 100\n",
    "}\n",
    "\n",
    "mt = MainTransformer()\n",
    "ft = FeatureTransformer()\n",
    "transformers = {'ft': ft}\n",
    "regressor_model = RegressorModel(model_wrapper=LGBWrapper_regr())\n",
    "regressor_model.fit(X=reduce_train, \n",
    "                    y=y, \n",
    "                    folds=folds, \n",
    "                    params=params, \n",
    "                    preprocesser=mt, \n",
    "                    transformers=transformers,\n",
    "                    eval_metric='cappa', \n",
    "                    cols_to_drop=cols_to_drop)\n",
    "\n",
    "preds_2 = regressor_model.predict(reduce_test)\n",
    "w_2 = LGB_BO.max['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del bounds_LGB, LGB_BO, params, mt, ft, transformers, regressor_model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blend and sumbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = (w_1/(w_1+w_2)) * preds_1 + (w_2/(w_1+w_2)) * preds_2\n",
    "\n",
    "del preds_1, preds_2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = [1.12232214, 1.73925866, 2.22506454]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[preds <= coefficients[0]] = 0\n",
    "preds[np.where(np.logical_and(preds > coefficients[0], preds <= coefficients[1]))] = 1\n",
    "preds[np.where(np.logical_and(preds > coefficients[1], preds <= coefficients[2]))] = 2\n",
    "preds[preds > coefficients[2]] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['accuracy_group'] = preds.astype(int)\n",
    "sample_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['accuracy_group'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
